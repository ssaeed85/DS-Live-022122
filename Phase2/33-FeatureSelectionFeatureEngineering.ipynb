{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Feature Selection and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We want to do our best to make good predictions by creating good models. One way we can improve our model is to consider the data's feature and either specifically _select_ features (**feature selection**) and/or _create new features_ (called **feature engineering**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Use correlations and other algorithms to inform feature selection\n",
    "- Address the problem of multicollinearity in regression problems\n",
    "- Create new features for use in modeling\n",
    "    - Use `PolynomialFeatures` to build compound features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New dataset for today! Insurance costs\n",
    "\n",
    "My source: https://www.kaggle.com/mirichoi0218/insurance (they got the idea for cleaning up the original open source data from [Machine Learning with R](https://www.packtpub.com/product/machine-learning-with-r-third-edition/9781788295864))\n",
    "\n",
    "Target: `charges`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualize relationships between numeric columns\n",
    "sns.pairplot(df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize correlations between numeric columns\n",
    "sns.heatmap(df.corr(), annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Things First - Train Test Split!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our X and y\n",
    "X = None\n",
    "y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "# Use test_size=0.25, random_state=42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to Encode!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our One Hot Encoder\n",
    "\n",
    "# Going to use ColumnTransformer to encode only cat cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which columns are categoricals?\n",
    "# Handy trick!\n",
    "\n",
    "cat_cols = [c for c in df.columns if df[c].dtype == 'O']\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an encoder object. This will help us to convert\n",
    "# categorical variables to new columns\n",
    "encoder = OneHotEncoder(handle_unknown='error',\n",
    "                        drop='first',\n",
    "                        categories='auto')\n",
    "\n",
    "# Create an columntransformer object.\n",
    "# This will help us to merge transformed columns\n",
    "# with the rest of the dataset.\n",
    "\n",
    "ct = ColumnTransformer(transformers=[('ohe', encoder, cat_cols)],\n",
    "                       remainder='passthrough')\n",
    "ct.fit(X_train)\n",
    "X_train_enc = ct.transform(X_train)\n",
    "X_test_enc = ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can display as a dataframe like so\n",
    "pd.DataFrame(X_train_enc, columns= ct.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now To Scale!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our scaler\n",
    "\n",
    "# scale our data\n",
    "\n",
    "# train on train data\n",
    "\n",
    "# transform both train and test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And let's model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our linear regression function\n",
    "\n",
    "# instantiate\n",
    "\n",
    "# fit\n",
    "\n",
    "# grab predictions for train and test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing our residuals \n",
    "# https://www.scikit-yb.org/en/latest/api/regressor/residuals.html\n",
    "from yellowbrick.regressor import ResidualsPlot\n",
    "\n",
    "visualizer = ResidualsPlot(lr)\n",
    "\n",
    "visualizer.fit(X_train_scaled, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test_scaled, y_test)  # Evaluate the model on the test data\n",
    "visualizer.show()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas to continue improving this model?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance through Coefficients\n",
    "\n",
    "Because we've scaled our data, we can explore our coefficients to see which are having more of an impact on our model.\n",
    "\n",
    "Note! This, or using p-values from a statsmodels model, is all you're expected to do in this project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make our scaled training data a df, for ease of use\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=ct.get_feature_names())\n",
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same with X_test_scaled\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=ct.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the coefficients with the names of each col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's model again using only the 3 strongest coefficients\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7622843807642006"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_subset.score(X_test_scaled[used_cols], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing our residuals\n",
    "# https://www.scikit-yb.org/en/latest/api/regressor/residuals.html\n",
    "\n",
    "visualizer = ResidualsPlot(lr_subset)\n",
    "\n",
    "visualizer.fit(X_train_scaled[used_cols], y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test_scaled[used_cols], y_test)  # Evaluate the model on the test data\n",
    "visualizer.show()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When do we need interaction terms? And how do we check for them?\n",
    "\n",
    "Well, first things first - what interactions do _you_ think would make sense? That's the easiest way to incorporate interaction terms - use domain knowledge to think through what usefully could be combined into an interaction.\n",
    "\n",
    "As for how to check if something might be better captured as an interaction..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the target back onto our OHE df\n",
    "# note the index difference...\n",
    "train_df = X_train_scaled.copy()\n",
    "train_df['target'] = y_train.reset_index(drop=True)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of no interaction term...\n",
    "sns.lmplot(x='age', y='target', hue='ohe__x1_yes', data=train_df, scatter=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do I know these two variables, `age` and `smoker_yes`, aren't interacting? \n",
    "\n",
    "- Look at the slopes - parallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's look at something else...\n",
    "sns.lmplot(x='bmi', y='target', hue='ohe__x1_yes', data=train_df, scatter=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Polynomial Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Instead of just multiplying features at random, we might consider trying **every possible product of features**. That's what PolynomialFeatures does.\n",
    "\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrating this on a toy example, with a single x variable predicting y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 150 samples from uniform distribution between -2pi and 2pi\n",
    "\n",
    "x = np.random.uniform(-2*np.pi, 2*np.pi, 150)\n",
    "\n",
    "# Creating target (y) - so we know the true relationship between x and y\n",
    "# But - adding some noise (error) with 'np.random'\n",
    "\n",
    "y = np.sin(x) + np.random.normal(loc=0, scale=0.4, size=len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize it\n",
    "plt.scatter(x, y)\n",
    "\n",
    "plt.ylabel('$\\sin(x)$ plus noise')\n",
    "plt.xlabel('x values are randomly chosen from $[-2\\pi, 2\\pi]$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a linear model\n",
    "lr = LinearRegression()\n",
    "lr.fit(x.reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the predicted values\n",
    "y_pred = lr.predict(x.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring our model\n",
    "lr.score(x.reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize it\n",
    "plt.scatter(x, y) # original data\n",
    "\n",
    "plt.plot(x, y_pred, c='red') # predicted values\n",
    "\n",
    "plt.ylabel('$\\sin(x)$ + noise')\n",
    "plt.xlabel('x values randomly chosen between $-2\\pi$ and $2\\pi$')\n",
    "plt.title(\"Simple Linear Regression\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this a good model? Well - of course not. It's definitely **underfit** - it is not complex enough to accurately capture the pattern and predict the target.\n",
    "\n",
    "Let's try again, but now with polynomials!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import polynomial features\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this, we'll need some helper functions\n",
    "# Shoutout to Andy for sending me these\n",
    "\n",
    "def create_poly_dataset(x, degree):\n",
    "    \"\"\"\n",
    "    returning dataset with the given polynomial degree\n",
    "    \"\"\"\n",
    "    # Instantiate the PolynomialFeatures object with given 'degree'\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Now transform data to create higher order features\n",
    "    new_data = poly.fit_transform(x.reshape(-1, 1))\n",
    "    return new_data\n",
    "\n",
    "def fit_linear_model(data, y):\n",
    "    \"\"\"\n",
    "    fitting a linear model and printing model details\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "    if data.ndim == 1:\n",
    "        data = data.reshape(-1, 1)\n",
    "\n",
    "    lr = LinearRegression(fit_intercept=False)\n",
    "    lr.fit(data, y)\n",
    "    print(\"-\"*13)\n",
    "    print(\"Coefficients: \", lr.coef_)\n",
    "    y_pred = lr.predict(data)\n",
    "    print(f\"R-Squared: {lr.score(data, y):.3f}\")\n",
    "    return lr\n",
    "\n",
    "def plot_predict(x, y, model):\n",
    "    \"\"\"\n",
    "    plotting predictions against true values\n",
    "    \"\"\"\n",
    "    plt.scatter(x, y, label='true')\n",
    "    x_pred = np.linspace(x.min(), x.max(), 100)\n",
    "    \n",
    "    # visualize beyond this x range by uncommenting below:\n",
    "#     extra = x.ptp() * .2\n",
    "#     x_pred = np.linspace(x.min() - extra, x.max() + extra, 100)\n",
    "\n",
    "    plt.plot(x_pred, model.predict(create_poly_dataset(x_pred, len(model.coef_)-1)),\n",
    "             label='predicted', c='red')\n",
    "\n",
    "    if len(model.coef_) == 1:\n",
    "        plt.title(f\"{len(model.coef_) - 1} Polynomial Terms \\n (no slope)\")\n",
    "    elif (len(model.coef_) - 1) == 1:\n",
    "        plt.title(f\"{len(model.coef_) - 1} Polynomial Term\")\n",
    "    else:\n",
    "        plt.title(f\"{len(model.coef_) - 1} Polynomial Terms\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualizing an assortment of polynomial degrees\n",
    "# can visualize each sequential polynomial with `range(n)`\n",
    "for i in [0, 1, 2, 3, 5, 7, 9, 13, 18]:\n",
    "    xi = create_poly_dataset(x, i)\n",
    "    plot_predict(x, y, fit_linear_model(xi, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate: which of these is the best?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate: so what?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First all polynomial terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction terms with Polynomial Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate: What do you think? Is this blanket way of approaching polynomial or interaction terms useful?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection and Feature Importances...\n",
    "\n",
    "Not much time to do this, but:\n",
    "\n",
    "- Lasso Regression (L1 regularization)\n",
    "- Recursive Feature Elimination\n",
    "- Forward Stepwise Selection\n",
    "\n",
    "Can always check out the python library [`eli5`](https://eli5.readthedocs.io/en/latest/index.html) (yes, Explain Like I'm 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources:\n",
    "\n",
    "[Feature Engineering and Selection: A Practical Approach for Predictive Models](https://bookdown.org/max/FES/) (computing done in R, but book focuses mostly on discussing the hows and whys rather than focusing on implementation)\n",
    "\n",
    "- Their chapter on [Encoding Categorical Predictors](https://bookdown.org/max/FES/encoding-categorical-predictors.html)\n",
    "- And their chapter on [Detecting Interaction Effects](https://bookdown.org/max/FES/detecting-interaction-effects.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
