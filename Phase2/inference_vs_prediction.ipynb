{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Inferential-Statistics-Vs.-Predictive-Statistics\" data-toc-modified-id=\"Inferential-Statistics-Vs.-Predictive-Statistics-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Inferential Statistics Vs. Predictive Statistics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Learning-Objectives\" data-toc-modified-id=\"Learning-Objectives-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Learning Objectives</a></span></li></ul></li><li><span><a href=\"#Inferential-Statistics-in-a-Nutshell\" data-toc-modified-id=\"Inferential-Statistics-in-a-Nutshell-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Inferential Statistics in a Nutshell</a></span></li><li><span><a href=\"#Predictive-Statistics-in-a-Nutshell\" data-toc-modified-id=\"Predictive-Statistics-in-a-Nutshell-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Predictive Statistics in a Nutshell</a></span><ul class=\"toc-item\"><li><span><a href=\"#In-Predictive-Statistics\" data-toc-modified-id=\"In-Predictive-Statistics-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>In Predictive Statistics</a></span></li></ul></li><li><span><a href=\"#Predictive-Modeling-Theory\" data-toc-modified-id=\"Predictive-Modeling-Theory-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Predictive Modeling Theory</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-Is-a-“Model”?\" data-toc-modified-id=\"What-Is-a-“Model”?-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>What Is a “Model”?</a></span></li><li><span><a href=\"#What-Makes-a-Model-Good?\" data-toc-modified-id=\"What-Makes-a-Model-Good?-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>What Makes a Model Good?</a></span></li><li><span><a href=\"#Return-to-Expected-Value\" data-toc-modified-id=\"Return-to-Expected-Value-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Return to Expected Value</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-expected-value-of-a-6-sided-die-is:\" data-toc-modified-id=\"The-expected-value-of-a-6-sided-die-is:-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>The expected value of a 6-sided die is:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Lets-Model-the-Die-Roll\" data-toc-modified-id=\"Lets-Model-the-Die-Roll-4.3.1.1\"><span class=\"toc-item-num\">4.3.1.1&nbsp;&nbsp;</span>Lets Model the Die Roll</a></span></li></ul></li></ul></li><li><span><a href=\"#Defining-model-bias-and-variance\" data-toc-modified-id=\"Defining-model-bias-and-variance-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Defining model bias and variance</a></span></li><li><span><a href=\"#Defining-Error:-prediction-error-and-irreducible-error\" data-toc-modified-id=\"Defining-Error:-prediction-error-and-irreducible-error-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Defining Error: prediction error and irreducible error</a></span><ul class=\"toc-item\"><li><span><a href=\"#Regression-fit-statistics-are-often-called-“error”\" data-toc-modified-id=\"Regression-fit-statistics-are-often-called-“error”-4.5.1\"><span class=\"toc-item-num\">4.5.1&nbsp;&nbsp;</span>Regression fit statistics are often called “error”</a></span></li><li><span><a href=\"#Exercise\" data-toc-modified-id=\"Exercise-4.5.2\"><span class=\"toc-item-num\">4.5.2&nbsp;&nbsp;</span>Exercise</a></span></li></ul></li><li><span><a href=\"#Defining-prediction-error-as-a-combination-of-bias-and-variance\" data-toc-modified-id=\"Defining-prediction-error-as-a-combination-of-bias-and-variance-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Defining prediction error as a combination of bias and variance</a></span></li></ul></li><li><span><a href=\"#Coming-up-next\" data-toc-modified-id=\"Coming-up-next-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Coming up next</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T16:03:50.968183Z",
     "start_time": "2022-03-24T16:03:50.961008Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inferential Statistics Vs. Predictive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Describe the hallmarks of inferential statistics, and to contrast them with the hallmarks of predictive statistics\n",
    "- Relate the goals of model-building to expected value, bias and variance\n",
    "- Define error as a function of prediction error and irreducible error\n",
    "- Define prediction error as a combination of bias and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inferential Statistics in a Nutshell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In Phase 1 we looked at **descriptive** statistics: starting with a dataset and making various observations (overall shape, histogram, outliers, etc.) as well as calculations of quantities that can characterize the dataset as a whole (mean, median, mode, variance, standard deviation, quartiles, percentiles, etc.).\n",
    "\n",
    "At the beginning of Phase 2 we moved into **inferential** statistics. The main idea here is to imagine that **we don't have** (or anyway cannot *measure*) all the data of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And this is, of course, the typical situation. Consider:\n",
    "\n",
    "- A zoologist wanting to know the typical lifespan of a Siberian tiger\n",
    "- A cosmologist wanting to know the mass of a normal white dwarf star\n",
    "- A businesswoman wanting to know how many M&M's her customers should expect to find in their Party Size bags\n",
    "- A botanist wanting to know how tall California redwoods usually grow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/tiger.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The zoologist could, in principle:\n",
    "\n",
    "1. keep track of every currently existing Siberian tiger\n",
    "2. record their (more or less) exact ages at their moments of death\n",
    "3. add up those ages and divide by the number of tigers to calculate an average lifespan\n",
    "\n",
    "––But **only** in principle. In all of these situations, there is no realistic or practical opportunity to check each relevant data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/sampling.png)\n",
    "\n",
    "What we can do, however, is to check *some* of the data points we want to check. That is, we'll draw a *sample* of data from our *population* of interest. We can then use the techniques of descriptive statistics to characterize our sample.\n",
    "\n",
    "The hope, then, is that\n",
    "- our sample will be *representative* of the population as a whole, \n",
    "- which would justify our using facts about the sample to ***infer*** things \n",
    "- Naturally we'll expect a certain amount of **error**: \n",
    "\n",
    "Inferential statistics makes all this precise. And that has been the bulk of the content of Phase 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Classically speaking, inference is a form of learning or of *increasing our knowledge*. So when conducting exercises in inferential statistics, the goal is ultimately **understanding**. If I am conducting a linear regression in an inferential mode, then:\n",
    "\n",
    "- I will be very interested in the values of the coefficients, since these represent the effect of the associated factors on the target in question\n",
    "- the more data I use to build the regression the better\n",
    "- the fewer transformations of my data the better, since lots of transformations will impede transparency and comprehensibility\n",
    "- fewer predictors may be better than more\n",
    "- I will be very interested in respecting the assumptions of linear regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Predictive Statistics in a Nutshell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The focus for predictive statistics is a bit different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First, the goal is less on understanding and more (of course!) on making good *predictions* of future cases.\n",
    "\n",
    "That means that I want the patterns I pick up on (in some dataset) to be patterns that will *recur* (in a similar dataset) in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/crystall_ball.png)\n",
    "\n",
    "<a href=\"https://commons.wikimedia.org/wiki/File:743-crystal-ball-1.svg\">Vincent Le Moign</a>, <a href=\"https://creativecommons.org/licenses/by/4.0\">CC BY 4.0</a>, via Wikimedia Commons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In Predictive Statistics\n",
    "- I won't particularly care about the values of the coefficients\n",
    "- I may want to have two different datasets: \n",
    "  - train the model\n",
    "  - test the model \n",
    "- I won't particularly care about whether or how the data has been modified or transformed before subjecting it to regression analysis\n",
    "- more predictors are probably better than fewer\n",
    "- I won't care as much about respecting the assumptions of linear regression\n",
    "- I'll probably choose `sklearn` if working in Python, since predictive statistics is at the heart of machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Of course, to the extent that we give up on actually trying to *understand* the phenomenon that we are modeling, to that extent we are happy to let our models be **black boxes**.\n",
    "- As we move deeper into the course and our models get ever more sophisticated, they will also become ever more like black boxes, for better or for worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**LESS MANUAL CODING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Predictive Modeling Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![which model is better](images/which_model_is_better.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What Is a “Model”?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A “model” is a general specification of relationships among variables. \n",
    "    + e.g. a linear regression, such as: $ Price = \\beta_1*Time +  \\beta_0 (+ \\epsilon)$\n",
    "- A “trained model” is a particular model that has been built using some training data.\n",
    "    + If the model is **parametric** (like a linear regression), then it has parameters that have been calculated using the training data;\n",
    "    + If the model is **non-parametric**, then it has (not parameters but) an algorithm that has been constructed using the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What Makes a Model Good?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We don’t ultimately care about how well your model fits your data.\n",
    "- What we really care about is how well your model describes the process that generated your data.\n",
    "- Why? Because the data set you have is but one sample from a universe of possible data sets, and you want a model that would work for any data set from that universe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Return to Expected Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The expected value of a quantity is the weighted average of that quantity across all possible samples\n",
    "\n",
    "![6 sided die](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/6sided_dice.jpg/600px-6sided_dice.jpg)\n",
    "\n",
    "- for a 6 sided die, another way to think about the expected value is the arithmetic mean of the rolls of a very large number of independent samples.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The expected value of a 6-sided die is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-23T16:15:09.626104Z",
     "start_time": "2022-03-23T16:15:09.594042Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = 1/6\n",
    "rolls = range(1, 7)\n",
    "\n",
    "expected_value = sum([probs * roll for roll in rolls])\n",
    "expected_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Lets Model the Die Roll\n",
    "\n",
    "Expected Value = 3.5<br>\n",
    "Guess Value    = Random Guess b/w 1 and 6<br>\n",
    "\n",
    "Roll = Die Roll Value\n",
    "\n",
    "Compute Diffs between Die Roll and \n",
    "   - Expected Value\n",
    "   - Guess Value \n",
    "   \n",
    "Over a large sample size, which will be closer?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T15:25:50.914811Z",
     "start_time": "2022-03-24T15:25:50.706483Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Diffs :  1.50343\n",
      "Guess   Diffs :  1.9513\n",
      "1 16644\n",
      "2 16653\n",
      "3 16620\n",
      "4 16570\n",
      "5 16707\n",
      "6 16806\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "expd = 3.0\n",
    "\n",
    "sumg=0\n",
    "sume=0\n",
    "nrolls=0\n",
    "toroll=100000\n",
    "dice = {}\n",
    "\n",
    "for xx in range(toroll):\n",
    "  roll = random.randint(1,6)\n",
    "  guess = random.randint(1,6)\n",
    "  diffg = guess-roll\n",
    "  diffe = expd - roll\n",
    "  sumg+=abs(diffg)\n",
    "  sume+=abs(diffe)\n",
    "  if roll in dice:\n",
    "      dice[roll]+=1\n",
    "  else:\n",
    "      dice[roll]=1\n",
    "  nrolls+=1\n",
    "    \n",
    "print(\"Expected Diffs : \",sume/nrolls)\n",
    "print(\"Guess   Diffs : \",sumg/nrolls)\n",
    "for dd in sorted(dice.keys()):\n",
    "    print(dd,dice[dd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining model bias and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's imagine we create a model that always predicts a roll of **3**.\n",
    "\n",
    "- **The *bias* is the difference between the average prediction of our model and the average roll of the die as we roll more and more times**.\n",
    "    - What is the bias of a model that always predicts 3?\n",
    "    \n",
    "- **The *variance* is the average difference between each individual prediction and the average prediction of our model as we roll more and more times**.\n",
    "    - What is the variance of that model?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining Error: prediction error and irreducible error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Regression fit statistics are often called “error”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Sum of Squared Errors (SSE)\n",
    "\n",
    " $ {\\displaystyle \\operatorname {SSE} =\\sum _{i=1}^{n}(Y_{i}-{\\hat {Y_{i}}})^{2}.} $\n",
    " \n",
    " \n",
    "- Mean Squared Error (MSE) \n",
    " \n",
    " $ {\\displaystyle \\operatorname {MSE} ={\\frac {1}{n}}\\sum _{i=1}^{n}(Y_{i}-{\\hat {Y_{i}}})^{2}.} $\n",
    " \n",
    " \n",
    "- Root Mean Squared Error (RMSE)  \n",
    "\n",
    " $ {\\displaystyle \\operatorname \n",
    "  {RMSE} =\\sqrt{MSE}} $\n",
    "\n",
    " All are calculated using residuals   $ (Y_{i}-{\\hat {Y_{i}}}) $\n",
    "\n",
    "![residuals](images/residuals.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - Fit a quick and dirty linear regression model\n",
    " - Store predictions in the y_hat variable using predict() from the fit model\n",
    " - Handcode SSE\n",
    " - Divide by the length of array to find Mean Squared Error\n",
    " - Check that your MSE equals sklearn's mean_squared_error function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T15:43:32.353772Z",
     "start_time": "2022-03-24T15:43:32.297038Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/king_county.csv', index_col='id')\n",
    "df = df.iloc[:, :12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T15:43:33.184138Z",
     "start_time": "2022-03-24T15:43:33.181326Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T15:59:04.864478Z",
     "start_time": "2022-03-24T15:59:04.821257Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop('price', axis=1)\n",
    "y = df.price\n",
    "\n",
    "# Build the regression\n",
    "lr = LinearRegression()\n",
    "y_hat = lr.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T16:51:47.994563Z",
     "start_time": "2022-03-24T16:51:47.986653Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate error\n",
    "y_hat = None\n",
    "sse = None\n",
    "mse = None\n",
    "rmse = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining prediction error as a combination of bias and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\Large Total\\ Error\\ = Prediction\\ Error+ Irreducible\\ Error$\n",
    "\n",
    "Our prediction error can be further broken down into error due to bias and error due to variance.\n",
    "\n",
    "$\\Large Total\\ Error = Model\\ Bias^2 + Model\\ Variance + Irreducible\\ Error$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Model Bias** is the expected prediction error of the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> In other words, if you were to train multiple models on different samples, what would be the average difference between the prediction and the real value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Model Variance** is the expected variation in predictions, relative to your trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> In other words, what would be the average difference between any one model's prediction and the average of all the predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Bias vs. variance refers ultimately to the *accuracy* vs. *consistency* of the models trained by your algorithm.**\n",
    "\n",
    "![target_bias_variance](images/target.png)\n",
    "\n",
    "http://scott.fortmann-roe.com/docs/BiasVariance.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bias vs. variance refers ultimately to the *accuracy* vs. *consistency* of the models trained by your algorithm.**\n",
    "\n",
    "![target_bias_variance](images/Bias-vs-Variance-v4.png)\n",
    "\n",
    "https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coming up next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It goes without saying that we would generally like our models to have both low bias and low variance. But what is not so obvious is that, unfortunately, as one tends to go down, the other tends to go up. Moreover, we shall often be able to tweak model **hyperparameters** with the purpose of decreasing the bias (even if that also means increasing the variance) or of decreasing the variance (even if that also means decreasing the bias). And so we shall soon come to appreciate the ***bias-variance tradeoff*** as it applies to machine learning models."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "345.797px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
